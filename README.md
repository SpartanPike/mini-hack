# ğŸ“¦ Hyper-Personalized Retail Support Agent  
### **(RAG + Tools + LLM Orchestration + PII Safety + Groq Integration)**

This repository contains a complete backend system for a **hyper-personalized conversational retail assistant**.  
It combines **RAG**, **real-time user context**, **tool integrations**, and **privacy-safe LLM reasoning** powered by **Groq Llama-3.3**.

---

# ğŸŒŸ What This System Does

This backend is designed for modern retail use-cases where customers expect:

- **Instant answers**
- **Personalized support**
- **Grounded responses**
- **Real-time recommendations**
- **Location-aware suggestions**

Example:

**User:** *â€œI'm cold.â€*  
**Bot:** *â€œThereâ€™s a Starbucks 50m away. You also have a 10% coupon for Hot Cocoa. Want directions?â€*

---

# ğŸ§  Key Features

## âœ”ï¸ Retrieval-Augmented Generation (RAG)
- Ingests multiple internal documents (policies, return rules, loyalty terms, coupons)
- Splits into chunks for optimized retrieval
- Embeds using `bge-small-en-v1.5`
- Stores in a **FAISS vector database**
- Retrieves relevant text for grounding every answer

## âœ”ï¸ Real-Time Retail Tools
The backend simulates real production tools:

- **Nearby Store Finder** (uses user GPS)
- **Personalized Coupon Engine**
- **Order Status Checker**
- **Inventory Availability Lookup**

These are included in each LLM prompt to create **actionable**, not generic, responses.

## âœ”ï¸ Privacy & PII Masking (Mandatory)
Before any data reaches the LLM, sensitive fields are masked:

- Phone numbers  
- Emails  
- Names  
- Addresses  
- Payment identifiers  

This ensures compliance and safe external LLM usage.

## âœ”ï¸ Orchestration Layer (Brain of the System)
A custom orchestrator handles:

1. Input validation  
2. Privacy filtering  
3. Tool execution  
4. RAG retrieval  
5. Prompt construction  
6. Groq LLM generation  
7. Final response formatting  

This ensures the agent is grounded, safe, and context-aware.

## âœ”ï¸ LLM Backend: Groq (FREE + FAST)
This project uses:

**`llama-3.3-70b-versatile` via GroqCloud**

Advantages:

- Free development tier  
- Extremely low latency  
- High-quality model outputs  
- No GPU/CPU dependency on your machine  

The LLM can be changed in:  
`src/config.py`

---

# ğŸ—ï¸ Architecture Overview

```
                â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                â”‚        User Query       â”‚
                â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                               â”‚
                               â–¼
                    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                    â”‚ FastAPI Backend   â”‚
                    â””â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â–¼                      â–¼
    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
    â”‚ PII Masking Layer   â”‚   â”‚ Tool Executor        â”‚
    â”‚ (privacy filter)    â”‚   â”‚ (stores, couponsâ€¦)   â”‚
    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                 â”‚                      â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚ RAG Retriever        â”‚
                 â”‚ (FAISS + embeddings) â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚ Prompt Orchestrator  â”‚
                 â”‚ (system + tools +    â”‚
                 â”‚   rag + user prompt) â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚ Groq LLM API         â”‚
                 â”‚ llama-3.3-70b        â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                            â”‚
                            â–¼
                 â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
                 â”‚   Final Response     â”‚
                 â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

# ğŸ“ Project Structure

```
hyper-personalized-support-bot/
â”‚
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ raw_docs/        # RAG source documents (.txt)
â”‚   â””â”€â”€ vector_store/    # FAISS index + metadata
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ rag/             # Ingestion + retrieval pipeline
â”‚   â”œâ”€â”€ models/          # Groq LLM wrapper + embeddings
â”‚   â”œâ”€â”€ app/             # FastAPI server + orchestration
â”‚   â”œâ”€â”€ utils/           # PII masking, helpers
â”‚   â””â”€â”€ config.py        # Global configuration
â”‚
â”œâ”€â”€ main.py              # FastAPI entrypoint
â””â”€â”€ README.md            # You're reading this
```

---

# ğŸš€ Getting Started

## 1ï¸âƒ£ Install Dependencies

```bash
pip install -r requirements.txt
```

## 2ï¸âƒ£ Add Your Groq API Key

Create `.env` in the project root:

```bash
GROQ_API_KEY=your_groq_key_here
```

## 3ï¸âƒ£ Build the Vector Database

Place policy documents in `data/raw_docs/`

Then run:

```bash
python -m src.rag.ingest
```

## 4ï¸âƒ£ Start the Server

```bash
python main.py
```

Server runs at:

```text
http://localhost:8000
```

API docs:

```text
http://localhost:8000/docs
```

---

# ğŸ§ª Example API Call

```bash
curl -X POST http://localhost:8000/chat   -H "Content-Type: application/json"   -d '{
    "user_id": "u123",
    "message": "What is the return policy?",
    "lat": 19.07,
    "lon": 72.87
  }'
```

Example response (real Llama-3.3-70B output via Groq):

```json
{
  "answer": "You can return unopened items within 30 days..."
}
```

---

# ğŸ”§ Configuration

Edit model or settings in:  
`src/config.py`

Example:

```python
LLM_MODEL_NAME = "llama-3.3-70b-versatile"
EMBEDDING_MODEL_NAME = "BAAI/bge-small-en-v1.5"
TOP_K = 5
```

---

# âš™ï¸ RAG Pipeline Details

1. Load .txt documents  
2. Split into overlapping chunks  
3. Embed each using BGE embeddings  
4. Index via FAISS  
5. At query time:
   - embed user question  
   - perform semantic search  
   - inject top-K chunks into the prompt  

This ensures the model **does not hallucinate** and stays aligned to policy.

---

# ğŸ” Privacy Enforcement

The system applies regex-based masking to:

- Names  
- Emails  
- Phone numbers  
- Addresses  
- Payment identifiers  

Masked before sending to Groq.  
This meets standard enterprise PII rules.

---

# ğŸ“¡ Tool Integrations

The orchestrator uses helper functions to simulate real retail systems:

- Distance to nearby stores  
- Available coupons  
- Latest order status  
- Possible inventory availability  

These are added under:

```text
### Real-Time Tools
```

in the final LLM prompt.

---

# ğŸ§© LLM Prompt Structure

The prompt is constructed as:

```text
System Instructions
User Profile
Real-Time Tools
RAG Context
User Message
```

This layering ensures:

- Accuracy  
- Personalization  
- Actionability  
- Grounded responses  

---

# ğŸ¯ Summary

This project demonstrates a **production-grade architecture** for Conversational Retail AI:

- ğŸ§  RAG-grounded responses  
- ğŸ”§ Real-time tools  
- ğŸ” PII-safe interactions  
- ğŸš€ Groq-powered LLM  
- âš¡ FastAPI backend  
- ğŸ’¬ Ready for chatbot UI integration  

It is designed to be easily extended with:

- React chat UI  
- Docker deployment  
- Production LLM endpoints  
- Additional tools (CRM, inventory, loyalty)  

---

# ğŸ—£ï¸ Next Step

You can now connect this backend to:

- A **React Chatbot UI**  
- A mobile app  
- A web widget  
- A kiosk interface  

